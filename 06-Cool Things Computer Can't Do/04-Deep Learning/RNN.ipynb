{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "> Humans don’t start their thinking from scratch every second. As you read/hear, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. *Your thoughts have persistence.*\n",
    "\n",
    "The sequence is a stream of data (finite or infinite) which are interdependent.\n",
    "\n",
    "- Examples would be time-series data, informative pieces of strings, conversations, etc.\n",
    "- In a conversation a sentence means something but the entire flow of the conversation mostly means something completely different. \n",
    "- In time-series data like stock market data, single tick data means the current price, but full days data will show movement and allow us to decide whether to buy or sell.\n",
    "\n",
    "CNN’s generally don’t perform well when the input data is interdependent in a sequential pattern. CNN’s don’t have any sort of correlation between the previous input to the next input. So all the outputs are self-dependent. \n",
    "\n",
    "RNNs have in them a sense of some memory about what happened earlier in the sequence of data. This helps the system to gain context. \n",
    "\n",
    "- They are the first of its kind State of the Art algorithms that can memorize/remember previous inputs in-memory when a huge set of sequential data is provided.\n",
    "\n",
    "Recurrent Neural Network remembers the past and its decisions are influenced by what it has learned from the past.\n",
    "\n",
    "RNNs can take one or more input vectors and produce one or more output vectors and the output(s) are influenced not just by weights applied on inputs like a regular NN, but also by a hidden state vector representing the context based on prior input(s)/output(s). So, the same input could produce a different output depending on previous inputs in the series.\n",
    "\n",
    "RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being dependent on the previous computations.\n",
    "\n",
    "![RNN](./images/rnn.webp)\n",
    "\n",
    "This chain-like nature unveils that recurrent neural networks are closely related to sequences and lists. They’re the natural structure of the neural network to use for such data.\n",
    "\n",
    "- RNN models are mostly used in the fields of natural language processing(NLP) and speech recognition.\n",
    "\n",
    "## RNN Intiution\n",
    "\n",
    "![RNN Intiution](./images/rnn_intiution.png)\n",
    "\n",
    "![RNN maths](./images/rnn_maths.png)\n",
    "\n",
    "![RNN Through Time](./images/rnn_through_time.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of RNN\n",
    "\n",
    "![RNN Types](./images/rnn-types.jpg)\n",
    "\n",
    "**One-to-one**: This is also called Plain Neural networks. It deals with a fixed size of the input to the fixed size of output, where they are independent of previous information/output.\n",
    "\n",
    "- Example: Image classification.\n",
    "\n",
    "\n",
    "**One-to-Many**: It deals with a fixed size of information as input that gives a sequence of data as output.\n",
    "\n",
    "- Example: Image Captioning takes the image as input and outputs a sentence of words.\n",
    "\n",
    "\n",
    "**Many-to-One**: It takes a sequence of information as input and outputs a fixed size of the output.\n",
    "\n",
    "- Example: sentiment analysis where any sentence is classified as expressing the positive or negative sentiment.\n",
    "\n",
    "\n",
    "**Many-to-Many**: It takes a Sequence of information as input and processes the recurrently outputs as a Sequence of data.\n",
    "\n",
    "- Example: Machine Translation, where the RNN reads any sentence in English and then outputs the sentence in French.\n",
    "\n",
    "\n",
    "**Bidirectional Many-to-Many**: Synced sequence input and output. Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.\n",
    "\n",
    "- Example: Video classification where we wish to label every frame of the video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Long-Short Term Memory)\n",
    "\n",
    "Forget Gate, Memory Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
