# Important Algorithms in Machine learning

**Explanatory algorithms** help us identify the variables that have a meaningful impact on the outcome we are interested in.

- These algorithms allow us to understand the relationships between the variables in the model, rather than just using the model to make predictions about the outcome.

- The algorithms in this category includes:
  - Linear Regression
  - Logistic Regression
  - Decision Trees
  - Principal Component Analysis (PCA)
  - Local Interpretable Model-Agnostic Explanations (LIME)
  - Shapley Additive explanations (SHAPLEY)
  - Shapley Approximation (SHAP)

**Pattern mining algorithms** are a type of data mining technique that are used to identify patterns and relationships within a dataset.

- They typically work by analyzing large datasets and looking for repeated patterns or associations between variables.
- Once these patterns have been identified, they can be used to make predictions about future trends or outcomes or to understand the underlying relationships within the data.
- The algorithms in this category includes:
  - Apriori algorithm
  - Recurrent Neural Network (RNN)
  - Long Short-Term Memory (LSTM)
  - Sequential Pattern Discovery Using Equivalence Class (SPADE)
  - PrefixSpan

**Ensemble learning algorithms** are machine learning techniques that combine the predictions of multiple models in order to make more accurate predictions than any of the individual models.

- These models can outperform traditional machine learning algorithms due:
  - Diversity: By combining the predictions of multiple models, ensemble algorithms can capture a wider range of patterns within the data.
  - Robustness: Ensemble algorithms are generally less sensitive to noise and outliers in the data, which can lead to more stable and reliable predictions.
  - Reducing overfitting: By averaging the predictions of multiple models, ensemble algorithms can reduce the tendency of individual models to overfit the training data, which can lead to improved generalization to new data.
  - Improved accuracy: Ensemble algorithms have been shown to consistently outperform traditional machine learning algorithms in a variety of contexts.
- The algorithms in this category includes:
  - Random Forest
  - XGBoost
  - LightGBM
  - CatBoost

**Clustering algorithms** are an unsupervised learning task and are used to group data into “clusters”.

- In contrast to supervised learning, where the target variable is known, there is no target variable in clustering.
- This technique is useful for finding natural patterns and trends in data and is often used during the exploratory data analysis phase to gain further understanding of the data.
- It can be used to divide a dataset into distinct segments based on various variables.
- The algorithms in this category includes:
  - K-mode clustering
  - DBSCAN
  - Spectral clustering

**Time series algorithms** are techniques used to analyze time-dependent data.

- These algorithms take into account the temporal dependencies among the data points in a series, which is especially important when trying to make predictions about future values.
- They can also be used to detect anomalies or changes in trends in the data.
- The algorithms in this category includes:
  - Prophet time series modelling
  - Autoregressive Integrated Moving Average (ARIMA)
  - Exponential smoothing

**Similarity algorithms** are used to measure the similarity between pairs of records, nodes, data points, or text.

- These algorithms can be based on the distance between two data points (e.g. Euclidean distance) or on the similarity of text (e.g. Levenshtein Algorithm).
- These algorithms have a wide range of applications, but are particularly useful in the context of recommendations.
- The algorithms in this category includes:
  - Euclidean Distance
  - Cosine Similarity
  - Levenshtein Algorithm
  - Jaro-Winkler Algorithm
  - Singular Value Decomposition (SVD)

# Machine Learning Models in detail

[**Linear Regression**](./Linear%20Regression.md)
[**Logistic Regression**](./Logistic%20Regression.md)
